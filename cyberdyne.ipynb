{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c7d6d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers datasets gradio tqdm ipywidgets huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c19c487",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# MiniLLM Training on Stanford OVAL Wikipedia\n",
    "# Seq2Seq, Gradio Demo - Optimized for Colab\n",
    "# With Streaming Mode Implementation\n",
    "# ===============================\n",
    "\n",
    "# First, install required packages\n",
    "print(\"Installing required packages...\")\n",
    "!pip install torch transformers datasets gradio tqdm huggingface_hub\n",
    "!pip install 'datasets[streaming]' aiohttp  # Required for streaming\n",
    "\n",
    "print(\"All packages installed successfully!\")\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "import gradio as gr\n",
    "import os\n",
    "import time\n",
    "from huggingface_hub import login\n",
    "from tqdm import tqdm\n",
    "from google.colab import userdata  # For accessing Colab secrets\n",
    "\n",
    "# -------------------------------\n",
    "# Authentication with Hugging Face\n",
    "# -------------------------------\n",
    "print(\"Authenticating with Hugging Face Hub...\")\n",
    "try:\n",
    "    # Try to get token from Colab secrets\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"Authentication successful using Colab secret!\")\n",
    "except Exception as e:\n",
    "    print(f\"Colab secret not found: {e}\")\n",
    "    try:\n",
    "        # Try environment variable\n",
    "        HF_TOKEN = os.environ.get('HF_TOKEN')\n",
    "        if HF_TOKEN:\n",
    "            login(token=HF_TOKEN)\n",
    "            print(\"Authentication successful using environment variable!\")\n",
    "        else:\n",
    "            print(\"No HF_TOKEN found. Continuing without authentication...\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Authentication failed: {e2}\")\n",
    "        print(\"Continuing without authentication - may have limited access\")\n",
    "\n",
    "# -------------------------------\n",
    "# Device detection\n",
    "# -------------------------------\n",
    "print(\"Detecting available hardware...\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Running on CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "# -------------------------------\n",
    "# Tokenizer\n",
    "# -------------------------------\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]', 'eos_token': ''})\n",
    "\n",
    "# -------------------------------\n",
    "# Streaming Dataset Implementation\n",
    "# -------------------------------\n",
    "print(\"Loading dataset with streaming...\")\n",
    "try:\n",
    "    # Load dataset with streaming enabled\n",
    "    dataset = load_dataset(\"stanford-oval/wikipedia\", \"20241101\", split=\"train\", streaming=True)\n",
    "\n",
    "    # Create a custom IterableDataset for streaming\n",
    "    class WikipediaStreamingDataset(IterableDataset):\n",
    "        def __init__(self, hf_dataset, tokenizer, max_samples=50000, max_len=256):\n",
    "            self.hf_dataset = hf_dataset\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_samples = max_samples\n",
    "            self.max_len = max_len\n",
    "\n",
    "        def __iter__(self):\n",
    "            count = 0\n",
    "            for item in self.hf_dataset:\n",
    "                if count >= self.max_samples:\n",
    "                    break\n",
    "\n",
    "                # Extract question and answer\n",
    "                question = item.get(\"document_title\", \"\")\n",
    "                answer = item.get(\"content\", \"\")\n",
    "\n",
    "                if question and answer:  # Only process if both exist\n",
    "                    # Tokenize inputs\n",
    "                    input_ids = self.tokenizer.encode(\n",
    "                        question,\n",
    "                        max_length=self.max_len//2,\n",
    "                        truncation=True,\n",
    "                        padding='max_length',\n",
    "                        return_tensors='pt'\n",
    "                    ).squeeze(0)\n",
    "\n",
    "                    # Tokenize targets with EOS token\n",
    "                    target_ids = self.tokenizer.encode(\n",
    "                        answer + self.tokenizer.eos_token,\n",
    "                        max_length=self.max_len//2,\n",
    "                        truncation=True,\n",
    "                        padding='max_length',\n",
    "                        return_tensors='pt'\n",
    "                    ).squeeze(0)\n",
    "\n",
    "                    yield input_ids, target_ids\n",
    "                    count += 1\n",
    "\n",
    "    # Create streaming dataset\n",
    "    streaming_dataset = WikipediaStreamingDataset(dataset, tokenizer)\n",
    "    print(\"Streaming dataset created successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading streaming dataset: {e}\")\n",
    "    print(\"Falling back to dummy data...\")\n",
    "    # Create dummy dataset as fallback\n",
    "    class DummyDataset(IterableDataset):\n",
    "        def __init__(self, tokenizer, num_samples=1000, max_len=256):\n",
    "            self.tokenizer = tokenizer\n",
    "            self.num_samples = num_samples\n",
    "            self.max_len = max_len\n",
    "\n",
    "        def __iter__(self):\n",
    "            for i in range(self.num_samples):\n",
    "                # Create dummy questions and answers\n",
    "                question = f\"What is topic {i}?\"\n",
    "                answer = f\"This is a detailed explanation about topic {i}.\"\n",
    "\n",
    "                input_ids = self.tokenizer.encode(\n",
    "                    question,\n",
    "                    max_length=self.max_len//2,\n",
    "                    truncation=True,\n",
    "                    padding='max_length',\n",
    "                    return_tensors='pt'\n",
    "                ).squeeze(0)\n",
    "\n",
    "                target_ids = self.tokenizer.encode(\n",
    "                    answer + self.tokenizer.eos_token,\n",
    "                    max_length=self.max_len//2,\n",
    "                    truncation=True,\n",
    "                    padding='max_length',\n",
    "                    return_tensors='pt'\n",
    "                ).squeeze(0)\n",
    "\n",
    "                yield input_ids, target_ids\n",
    "\n",
    "    streaming_dataset = DummyDataset(tokenizer)\n",
    "    print(\"Using dummy dataset as fallback\")\n",
    "\n",
    "# -------------------------------\n",
    "# MiniLLM Model\n",
    "# -------------------------------\n",
    "class MiniLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size=512, n_layers=8, n_heads=8, ff_size=2048, max_len=256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_size)\n",
    "        self.pos_embed = nn.Embedding(max_len, emb_size)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=emb_size,\n",
    "                nhead=n_heads,\n",
    "                dim_feedforward=ff_size,\n",
    "                activation='gelu',\n",
    "                batch_first=True\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln = nn.LayerNorm(emb_size)\n",
    "        self.head = nn.Linear(emb_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0)\n",
    "        x = self.embed(x) + self.pos_embed(positions)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "model = MiniLLM(vocab_size)\n",
    "model.to(device)\n",
    "print(\"MiniLLM initialized with vocab size:\", vocab_size)\n",
    "\n",
    "# -------------------------------\n",
    "# Training Setup\n",
    "# -------------------------------\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "num_epochs = 3  # Reduced for demonstration\n",
    "target_batches = 1000  # Reduced target for demonstration\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 4 if torch.cuda.is_available() else 2\n",
    "loader = DataLoader(streaming_dataset, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs with target of {target_batches} batches...\")\n",
    "print(f\"Using batch size: {batch_size}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Training Loop\n",
    "# -------------------------------\n",
    "start_time = time.time()\n",
    "batch_count = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(progress_bar):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        logits = logits.view(-1, vocab_size)\n",
    "        y_flat = y.view(-1)\n",
    "        loss = loss_fn(logits, y_flat)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_count += 1\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        # Check if we've reached the target number of batches\n",
    "        if batch_count >= target_batches:\n",
    "            break\n",
    "\n",
    "    # Calculate average epoch loss\n",
    "    avg_epoch_loss = epoch_loss / (batch_idx + 1)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed. Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save(model.state_dict(), f\"minillm_epoch_{epoch+1}.pt\")\n",
    "    print(f\"Checkpoint saved for epoch {epoch+1}\")\n",
    "\n",
    "    if batch_count >= target_batches:\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training completed in {training_time/60:.2f} minutes!\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"minillm_final.pt\")\n",
    "print(\"Final model saved.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Gradio Demo\n",
    "# -------------------------------\n",
    "def answer_question(question, max_new_tokens=100, top_k=50, temperature=0.8):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(question, return_tensors=\"pt\").to(device)\n",
    "    generated = tokens\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = model(generated)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            probs = torch.softmax(next_token_logits / temperature, dim=-1)\n",
    "            top_probs, top_idx = torch.topk(probs, top_k)\n",
    "            next_token = top_idx[0, torch.multinomial(top_probs[0], 1)]\n",
    "            generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    output = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    return output\n",
    "\n",
    "# Create and launch Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=gr.Textbox(label=\"Question\"),\n",
    "    outputs=gr.Textbox(label=\"Answer\"),\n",
    "    title=\"MiniLLM Question Answering Demo\",\n",
    "    description=\"Ask a question and see how the MiniLLM model responds!\"\n",
    ")\n",
    "\n",
    "demo.launch(share=True)  # share=True creates a public link\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
